{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation for porting to other platforms and discussion (this is not to do exploratory analysis but rather to consider the APIs and technologies involved - it is not intended to be a good or reference solution to this problem). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the data from Google Cloud Storage buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-29 09:03:38--  https://storage.googleapis.com/bdt-spark-store/external_sources.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.223.144, 172.217.170.16\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.223.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15503836 (15M) [text/csv]\n",
      "Saving to: ‘gcs_external_sources.csv’\n",
      "\n",
      "gcs_external_source 100%[===================>]  14.79M  2.46MB/s    in 6.5s    \n",
      "\n",
      "2020-11-29 09:03:46 (2.28 MB/s) - ‘gcs_external_sources.csv’ saved [15503836/15503836]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/bdt-spark-store/external_sources.csv -O gcs_external_sources.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-29 09:03:50--  https://storage.googleapis.com/bdt-spark-store/internal_data.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.223.144, 172.217.170.16\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.223.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152978396 (146M) [text/csv]\n",
      "Saving to: ‘gcs_internal_data.csv’\n",
      "\n",
      "gcs_internal_data.c 100%[===================>] 145.89M  2.46MB/s    in 61s     \n",
      "\n",
      "2020-11-29 09:04:52 (2.39 MB/s) - ‘gcs_internal_data.csv’ saved [152978396/152978396]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/bdt-spark-store/internal_data.csv -O gcs_internal_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# import pandas as pd \n",
    "spark = SparkSession.builder.appName('panda-and-spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = pd.read_csv('gcs_internal_data.csv')\n",
    "df_data = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"gcs_internal_data.csv\")\n",
    "# df_ext = pd.read_csv('gcs_external_sources.csv')\n",
    "df_ext = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"gcs_external_sources.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join them on their common identifier key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_data.join(df_ext, df_ext.SK_ID_CURR == df_data.SK_ID_CURR)\n",
    "# df_full.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter a few features out for the sake of this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
    "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
    "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
    "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
    "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
    "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
    "categorical_columns= ['NAME_INCOME_TYPE','ORGANIZATION_TYPE','CODE_GENDER','NAME_EDUCATION_TYPE']\n",
    "non_cate_cols= [i for i in columns_extract if i not in categorical_columns ]\n",
    "\n",
    "df = df_full[columns_extract]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain a train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8,0.2],seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246057"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()-test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|TARGET| count|  Value_split_train|\n",
      "+------+------+-------------------+\n",
      "|     0|226229| 0.9194170456438955|\n",
      "|     1| 19828|0.08058295435610448|\n",
      "+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zz=train[['TARGET']].groupby('TARGET').count()\n",
    "zz.withColumn('Value_split_train',zz['count']/train.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|TARGET|count|   Value_split_test|\n",
      "+------+-----+-------------------+\n",
      "|     0|56457|  0.918687148110782|\n",
      "|     1| 4997|0.08131285188921795|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zz1=test[['TARGET']].groupby('TARGET').count()\n",
    "zz1.withColumn('Value_split_test',zz1['count']/test.count()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the train set \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "model=pipeline.fit(train)\n",
    "transformed = model.transform(train)\n",
    "# transformed.show(5)\n",
    "\n",
    "actualCol = transformed.columns\n",
    "newCols=[]\n",
    "#Get c and its repective index. One hot encoder will put those on same index in vector\n",
    "for i in categorical_columns:\n",
    "    colIdx = transformed.select(i,i+\"_indexed\").distinct().rdd.collectAsMap()\n",
    "    colIdx =  sorted((value, i+\"_\" + key) for (key, value) in colIdx.items())\n",
    "    newCols += list(map(lambda x: x[1], colIdx))\n",
    "#     allColNames += newCols\n",
    "allColNames = actualCol+newCols\n",
    "\n",
    "def extract(row):\n",
    "    return tuple(map(lambda x: row[x], row.__fields__)) + tuple(row.NAME_INCOME_TYPE_indexed_encoded.toArray().tolist())+ tuple(row.ORGANIZATION_TYPE_indexed_encoded.toArray().tolist())+ tuple(row.CODE_GENDER_indexed_encoded.toArray().tolist())+ tuple(row.NAME_EDUCATION_TYPE_indexed_encoded.toArray().tolist())\n",
    "\n",
    "result = transformed.rdd.map(extract).toDF(allColNames,sampleRatio=0.2)\n",
    "for col in newCols:\n",
    "    result = result.withColumn(col, result[col].cast(\"int\"))\n",
    "final_df_cols=non_cate_cols+newCols\n",
    "res_train=result[final_df_cols]\n",
    "# res2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the test set \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "model=pipeline.fit(test)\n",
    "transformed = model.transform(test)\n",
    "# transformed.show(5)\n",
    "\n",
    "actualCol = transformed.columns\n",
    "newCols=[]\n",
    "#Get c and its repective index. One hot encoder will put those on same index in vector\n",
    "for i in categorical_columns:\n",
    "    colIdx = transformed.select(i,i+\"_indexed\").distinct().rdd.collectAsMap()\n",
    "    colIdx =  sorted((value, i+\"_\" + key) for (key, value) in colIdx.items())\n",
    "    newCols += list(map(lambda x: x[1], colIdx))\n",
    "#     allColNames += newCols\n",
    "allColNames = actualCol+newCols\n",
    "\n",
    "def extract(row):\n",
    "    return tuple(map(lambda x: row[x], row.__fields__)) + tuple(row.NAME_INCOME_TYPE_indexed_encoded.toArray().tolist())+ tuple(row.ORGANIZATION_TYPE_indexed_encoded.toArray().tolist())+ tuple(row.CODE_GENDER_indexed_encoded.toArray().tolist())+ tuple(row.NAME_EDUCATION_TYPE_indexed_encoded.toArray().tolist())\n",
    "\n",
    "result = transformed.rdd.map(extract).toDF(allColNames,sampleRatio=0.2)\n",
    "for col in newCols:\n",
    "    result = result.withColumn(col, result[col].cast(\"int\"))\n",
    "final_df_cols=non_cate_cols+newCols\n",
    "res_test=result[final_df_cols]\n",
    "\n",
    "# res2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test', 61454, 88)\n",
      "('train', 246057, 88)\n"
     ]
    }
   ],
   "source": [
    "print((\"test\",res_test.count(), len(res_test.columns)))\n",
    "print((\"train\",res_train.count(), len(res_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the training and test data (as the test data may not have the same columns in the encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test', 61454, 88)\n",
      "('train', 246057, 88)\n"
     ]
    }
   ],
   "source": [
    "final_features=list(set(res_train.schema.names).intersection(res_train.schema.names))\n",
    "train=res_train[final_features]\n",
    "test=res_test[final_features]\n",
    "\n",
    "print((\"test\",test.count(), len(test.columns)))\n",
    "print((\"train\",train.count(), len(train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labels from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['TARGET']\n",
    "test_labels = test['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing data and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the target from the training data -- cannot implement otherwise the random forest will not work\n",
    "# if 'TARGET' in train.schema.names:\n",
    "#     train = train.drop(*['TARGET'])\n",
    "#     test = test.drop(*['TARGET'])\n",
    "\n",
    "    \n",
    "# # Feature names\n",
    "# features = list(train.schema.names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item[0] for item in train.dtypes if item[1].startswith('string') ]\n",
    "\n",
    "for col in ['EXT_SOURCE_3',\n",
    " 'OWN_CAR_AGE',\n",
    " 'AMT_ANNUITY',\n",
    " 'AMT_CREDIT',\n",
    " 'DAYS_BIRTH',\n",
    " 'DAYS_ID_PUBLISH',\n",
    " 'DAYS_REGISTRATION',\n",
    " 'DAYS_EMPLOYED',\n",
    " 'EXT_SOURCE_1',\n",
    " 'AMT_GOODS_PRICE',\n",
    " 'DAYS_LAST_PHONE_CHANGE',\n",
    " 'AMT_INCOME_TOTAL',\n",
    " 'EXT_SOURCE_2','TARGET']:\n",
    "    test = test.withColumn(col, test[col].cast(\"int\"))\n",
    "    train = train.withColumn(col, train[col].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer= Imputer(inputCols=train.schema.names, outputCols=train.schema.names )\n",
    "\n",
    "train=imputer.fit(train).transform(train)\n",
    "test=imputer.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "columns_to_scale = ['EXT_SOURCE_3',\n",
    " 'OWN_CAR_AGE',\n",
    " 'AMT_ANNUITY',\n",
    " 'AMT_CREDIT',\n",
    " 'DAYS_BIRTH',\n",
    " 'DAYS_ID_PUBLISH',\n",
    " 'DAYS_REGISTRATION',\n",
    " 'DAYS_EMPLOYED',\n",
    " 'EXT_SOURCE_1',\n",
    " 'AMT_GOODS_PRICE',\n",
    " 'DAYS_LAST_PHONE_CHANGE',\n",
    " 'AMT_INCOME_TOTAL',\n",
    " 'EXT_SOURCE_2']\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + '_Vec' ) for col in columns_to_scale]\n",
    "scalers = [StandardScaler(inputCol=col+ '_Vec'  , outputCol=col + '_Scaled' ) for col in columns_to_scale]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = scalerModel.transform(train)\n",
    "test = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[]\n",
    "for i in ['EXT_SOURCE_3',\n",
    " 'OWN_CAR_AGE',\n",
    " 'AMT_ANNUITY',\n",
    " 'AMT_CREDIT',\n",
    " 'DAYS_BIRTH',\n",
    " 'DAYS_ID_PUBLISH',\n",
    " 'DAYS_REGISTRATION',\n",
    " 'DAYS_EMPLOYED',\n",
    " 'EXT_SOURCE_1',\n",
    " 'AMT_GOODS_PRICE',\n",
    " 'DAYS_LAST_PHONE_CHANGE',\n",
    " 'AMT_INCOME_TOTAL',\n",
    " 'EXT_SOURCE_2']:\n",
    "    new+=[i]\n",
    "    new+=[i+'_Vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(*new)\n",
    "test=test.drop(*new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "oldColumns = train.schema.names\n",
    "newColumns = ['NAME_INCOME_TYPE_Businessman',\n",
    " 'ORGANIZATION_TYPE_Services',\n",
    " 'ORGANIZATION_TYPE_Industry: type 8',\n",
    " 'ORGANIZATION_TYPE_Agriculture',\n",
    " 'NAME_EDUCATION_TYPE_Higher education',\n",
    " 'ORGANIZATION_TYPE_Transport: type 3',\n",
    " 'NAME_EDUCATION_TYPE_Secondary / secondary special',\n",
    " 'ORGANIZATION_TYPE_Trade: type 4',\n",
    " 'ORGANIZATION_TYPE_Emergency',\n",
    " 'NAME_INCOME_TYPE_State servant',\n",
    " 'ORGANIZATION_TYPE_Business Entity Type 3',\n",
    " 'ORGANIZATION_TYPE_Advertising',\n",
    " 'ORGANIZATION_TYPE_Industry: type 1',\n",
    " 'ORGANIZATION_TYPE_Construction',\n",
    " 'ORGANIZATION_TYPE_Industry: type 7',\n",
    " 'ORGANIZATION_TYPE_Postal',\n",
    " 'ORGANIZATION_TYPE_Electricity',\n",
    " 'ORGANIZATION_TYPE_University',\n",
    " 'ORGANIZATION_TYPE_Restaurant',\n",
    " 'ORGANIZATION_TYPE_School',\n",
    " 'ORGANIZATION_TYPE_Industry: type 5',\n",
    " 'ORGANIZATION_TYPE_Other',\n",
    " 'NAME_EDUCATION_TYPE_Lower secondary',\n",
    " 'ORGANIZATION_TYPE_Industry: type 9',\n",
    " 'ORGANIZATION_TYPE_Industry: type 4',\n",
    " 'NAME_INCOME_TYPE_Student',\n",
    " 'ORGANIZATION_TYPE_Medicine',\n",
    " 'NAME_INCOME_TYPE_Maternity leave',\n",
    " 'ORGANIZATION_TYPE_Industry: type 6',\n",
    " 'ORGANIZATION_TYPE_Trade: type 5',\n",
    " 'ORGANIZATION_TYPE_Mobile',\n",
    " 'ORGANIZATION_TYPE_Housing',\n",
    " 'ORGANIZATION_TYPE_Military',\n",
    " 'ORGANIZATION_TYPE_Trade: type 6',\n",
    " 'ORGANIZATION_TYPE_Security',\n",
    " 'ORGANIZATION_TYPE_Insurance',\n",
    " 'CODE_GENDER_F',\n",
    " 'ORGANIZATION_TYPE_Transport: type 2',\n",
    " 'TARGET',\n",
    " 'NAME_EDUCATION_TYPE_Incomplete higher',\n",
    " 'NAME_INCOME_TYPE_Working',\n",
    " 'ORGANIZATION_TYPE_Trade: type 3',\n",
    " 'ORGANIZATION_TYPE_Police',\n",
    " 'ORGANIZATION_TYPE_Trade: type 1',\n",
    " 'ORGANIZATION_TYPE_Business Entity Type 1',\n",
    " 'CODE_GENDER_M',\n",
    " 'ORGANIZATION_TYPE_Business Entity Type 2',\n",
    " 'ORGANIZATION_TYPE_Realtor',\n",
    " 'ORGANIZATION_TYPE_Trade: type 7',\n",
    " 'ORGANIZATION_TYPE_Transport: type 4',\n",
    " 'ORGANIZATION_TYPE_Industry: type 11',\n",
    " 'ORGANIZATION_TYPE_Transport: type 1',\n",
    " 'ORGANIZATION_TYPE_Hotel',\n",
    " 'ORGANIZATION_TYPE_Bank',\n",
    " 'ORGANIZATION_TYPE_Kindergarten',\n",
    " 'ORGANIZATION_TYPE_Cleaning',\n",
    " 'CODE_GENDER_XNA',\n",
    " 'NAME_INCOME_TYPE_Commercial associate',\n",
    " 'ORGANIZATION_TYPE_Industry: type 12',\n",
    " 'ORGANIZATION_TYPE_Legal Services',\n",
    " 'ORGANIZATION_TYPE_Industry: type 2',\n",
    " 'ORGANIZATION_TYPE_Industry: type 13',\n",
    " 'NAME_INCOME_TYPE_Unemployed',\n",
    " 'ORGANIZATION_TYPE_Culture',\n",
    " 'ORGANIZATION_TYPE_Security Ministries',\n",
    " 'NAME_INCOME_TYPE_Pensioner',\n",
    " 'ORGANIZATION_TYPE_Religion',\n",
    " 'ORGANIZATION_TYPE_Industry: type 10',\n",
    " 'ORGANIZATION_TYPE_Telecom',\n",
    " 'ORGANIZATION_TYPE_Trade: type 2',\n",
    " 'ORGANIZATION_TYPE_XNA',\n",
    " 'NAME_EDUCATION_TYPE_Academic degree',\n",
    " 'ORGANIZATION_TYPE_Industry: type 3',\n",
    " 'ORGANIZATION_TYPE_Government',\n",
    " 'ORGANIZATION_TYPE_Self-employed',\n",
    " 'EXT_SOURCE_3',\n",
    " 'OWN_CAR_AGE',\n",
    " 'AMT_ANNUITY',\n",
    " 'AMT_CREDIT',\n",
    " 'DAYS_BIRTH',\n",
    " 'DAYS_ID_PUBLISH',\n",
    " 'DAYS_REGISTRATION',\n",
    " 'DAYS_EMPLOYED',\n",
    " 'EXT_SOURCE_1',\n",
    " 'AMT_GOODS_PRICE',\n",
    " 'DAYS_LAST_PHONE_CHANGE',\n",
    " 'AMT_INCOME_TOTAL',\n",
    " 'EXT_SOURCE_2']\n",
    "\n",
    "train = reduce(lambda train, idx: train.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), train)\n",
    "\n",
    "oldColumns = test.schema.names\n",
    "test = reduce(lambda test, idx: test.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test', 61454, 88)\n",
      "('train', 246057, 88)\n"
     ]
    }
   ],
   "source": [
    "print((\"test\",test.count(), len(test.columns)))\n",
    "print((\"train\",train.count(), len(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list=[i for i in final_features if i not in ['TARGET']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_list,outputCol=\"features\")\n",
    "\n",
    "\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol='features', numTrees=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler , rf])\n",
    "\n",
    "model=pipeline.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=model.transform(train)\n",
    "test_pred=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.918687\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"TARGET\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_pred)\n",
    "print(\"Test Accuracy = %g\" % (accuracy))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
